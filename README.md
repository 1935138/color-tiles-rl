# Color Tiles κ°•ν™”ν•™μµ ν”„λ΅μ νΈ

Color Tiles κ²μ„μ„ ν”λ μ΄ν•λ” κ°•ν™”ν•™μµ AIλ¥Ό ν•™μµν•κ³  μ‹¤ν–‰ν•λ” ν”„λ΅μ νΈμ…λ‹λ‹¤.

## ν”„λ΅μ νΈ κ°μ”

- **κ²μ„**: Color Tiles (23Γ—15 λ³΄λ“, 10κ°€μ§€ μƒ‰μƒ, 200κ° νƒ€μΌ)
- **μ•κ³ λ¦¬μ¦**: PPO (Proximal Policy Optimization)
- **ν”„λ μ„μ›ν¬**: Stable-Baselines3 + Gymnasium
- **GUI**: PyQt6

## μ„¤μΉ λ°©λ²•

### 1. κΈ°λ³Έ μμ΅΄μ„± μ„¤μΉ

```bash
# Python 3.8 μ΄μƒ ν•„μ”
pip install -e .
```

### 2. κ°•ν™”ν•™μµ μμ΅΄μ„± μ„¤μΉ

```bash
pip install gymnasium stable-baselines3 torch tensorboard
```

λλ”:

```bash
pip install -r requirements-rl.txt
```

## μ‚¬μ© λ°©λ²•

### 1. GUIλ΅ κ²μ„ ν”λ μ΄ (μ‚¬λ)

```bash
python main.py
```

κΈ°λ³Έ κ²μ„ ν”λ μ΄:
1. "κ²μ„ μ‹μ‘" λ²„νΌ ν΄λ¦­
2. λΉ μ…€μ„ ν΄λ¦­ν•μ—¬ νƒ€μΌ μ κ±°
3. 120μ΄ λ‚΄μ— λ¨λ“  νƒ€μΌ μ κ±° μ‹ μΉλ¦¬!

---

### 2. AI ν•™μµ

#### μ§§μ€ ν…μ¤νΈ ν•™μµ (10K steps)

```bash
python -m rl.training.train --total-timesteps 10000 --n-envs 2
```

μ΄ λ…λ Ήμ–΄λ”:
- 10,000 timesteps λ™μ• ν•™μµ
- 2κ°μ λ³‘λ ¬ ν™κ²½ μ‚¬μ©
- μ•½ 5-10λ¶„ μ†μ” (CPU κΈ°μ¤€)
- μ²΄ν¬ν¬μΈνΈλ¥Ό `checkpoints/` λ””λ ‰ν† λ¦¬μ— μ €μ¥

#### λ³Έκ²© ν•™μµ (1M steps)

```bash
python -m rl.training.train --total-timesteps 1000000 --n-envs 8
```

μ΄ λ…λ Ήμ–΄λ”:
- 1,000,000 timesteps λ™μ• ν•™μµ
- 8κ°μ λ³‘λ ¬ ν™κ²½ μ‚¬μ©
- μ•½ 10-20μ‹κ°„ μ†μ” (CPU κΈ°μ¤€)
- λ§¤ 10,000 stepsλ§λ‹¤ μ²΄ν¬ν¬μΈνΈ μ €μ¥
- λ§¤ 5,000 stepsλ§λ‹¤ ν‰κ°€ μν–‰

#### ν•™μµ νλΌλ―Έν„°

```bash
python -m rl.training.train \
  --total-timesteps 1000000 \
  --n-envs 8 \
  --learning-rate 3e-4 \
  --seed 42 \
  --save-dir checkpoints
```

**νλΌλ―Έν„° μ„¤λ…:**
- `--total-timesteps`: μ΄ ν•™μµ μ¤ν… μ (κΈ°λ³Έκ°’: 1,000,000)
- `--n-envs`: λ³‘λ ¬ ν™κ²½ κ°μ (κΈ°λ³Έκ°’: 8)
- `--learning-rate`: ν•™μµλ¥  (κΈ°λ³Έκ°’: 3e-4)
- `--seed`: λλ¤ μ‹λ“ (κΈ°λ³Έκ°’: 0)
- `--save-dir`: μ²΄ν¬ν¬μΈνΈ μ €μ¥ λ””λ ‰ν† λ¦¬ (κΈ°λ³Έκ°’: checkpoints)

#### ν•™μµ μ¬κ° (μ²΄ν¬ν¬μΈνΈμ—μ„)

```bash
python -m rl.training.train \
  --checkpoint checkpoints/ppo_colortiles_step_50000.zip \
  --total-timesteps 1000000
```

---

### 3. ν•™μµ λ¨λ‹ν„°λ§ (TensorBoard)

```bash
tensorboard --logdir logs/tensorboard/
```

κ·Έλ° λ‹¤μ λΈλΌμ°μ €μ—μ„ http://localhost:6006 μ ‘μ†

**ν™•μΈ κ°€λ¥ν• μ§€ν‘:**
- Episode reward (μ—ν”Όμ†λ“ λ³΄μƒ)
- Win rate (μΉλ¦¬ λΉ„μ¨)
- Mean tiles cleared (ν‰κ·  μ κ±° νƒ€μΌ μ)
- Episode length (μ—ν”Όμ†λ“ κΈΈμ΄)
- Policy loss, Value loss
- Entropy (νƒν— μ •λ„)

---

### 4. GUIμ—μ„ ν•™μµλ AI ν”λ μ΄

#### Step 1: GUI μ‹¤ν–‰

```bash
python main.py
```

#### Step 2: AI μ„¤μ •

1. **μ²΄ν¬ν¬μΈνΈ μ„ νƒ**:
   - μ°μΈ΅ "AI ν”λ μ΄μ–΄" ν¨λ„μ—μ„ μ²΄ν¬ν¬μΈνΈ λ“λ΅­λ‹¤μ΄ ν΄λ¦­
   - ν•™μµλ λ¨λΈ μ„ νƒ (μ: `ppo_colortiles_best.zip`)
   - "μƒλ΅κ³ μΉ¨" λ²„νΌμΌλ΅ λ©λ΅ κ°±μ‹  κ°€λ¥

2. **AI μ‹μ‘**:
   - "AI μ‹μ‘" λ²„νΌ ν΄λ¦­
   - κ²μ„μ΄ μλ™ μ‹μ‘λκ³  AIκ°€ ν”λ μ΄ μ‹μ‘

3. **μ†λ„ μ΅°μ **:
   - μ†λ„ μ¬λΌμ΄λ”λ΅ 1-10 μ΅°μ  (μ΄λ‹Ή μ•΅μ… μ)
   - 1: λλ¦Ό (κ΄€μ°° μ©μ΄)
   - 10: λΉ λ¦„

4. **AI μ¤‘μ§€**:
   - "μ¤‘μ§€" λ²„νΌμΌλ΅ μ–Έμ λ“ μ§€ μ¤‘μ§€ κ°€λ¥

#### AI μƒνƒ μ •λ³΄

GUIμ—μ„ λ‹¤μ μ •λ³΄ ν™•μΈ κ°€λ¥:
- **μ¤ν…**: ν„μ¬ μ—ν”Όμ†λ“μ μ¤ν… μ
- **κ°€μΉ μ¶”μ •**: AIκ°€ μμΈ΅ν• state value
- **ν–‰λ™ μ‹ λΆ°λ„**: μ„ νƒν• μ•΅μ…μ ν™•λ¥ 
- **λ‹¤μ ν–‰λ™**: AIκ°€ μ„ νƒν•  μ„μΉ (row, col)
- **ν•μ΄λΌμ΄νΈ**: λ³΄λ“μ—μ„ λ‹¤μ μ•΅μ… μ„μΉλ¥Ό μƒ‰μƒμΌλ΅ ν‘μ‹
  - πΆ λ…Ήμƒ‰: λ†’μ€ μ‹ λΆ°λ„ (>80%)
  - π΅ λ…Έλ€μƒ‰: μ¤‘κ°„ μ‹ λΆ°λ„ (50-80%)
  - π  μ£Όν™©μƒ‰: λ‚®μ€ μ‹ λΆ°λ„ (<50%)

---

## ν”„λ΅μ νΈ κµ¬μ΅°

```
color-tiles-rl/
β”β”€β”€ src/
β”‚   β”β”€β”€ color_tiles/          # κ²μ„ μ—”μ§„
β”‚   β”‚   β”β”€β”€ domain/           # λ„λ©”μΈ λ¨λΈ (Color, Position, GameState)
β”‚   β”‚   β”β”€β”€ engine/           # κ²μ„ λ΅μ§ (Board, GameEngine)
β”‚   β”‚   β”β”€β”€ gui/              # PyQt6 GUI
β”‚   β”‚   β”‚   β”β”€β”€ main_window.py
β”‚   β”‚   β”‚   β”β”€β”€ board_widget.py
β”‚   β”‚   β”‚   β”β”€β”€ ai_control_panel.py   # AI μ μ–΄ ν¨λ„
β”‚   β”‚   β”‚   β””β”€β”€ ai_status_panel.py    # AI μƒνƒ ν‘μ‹
β”‚   β”‚   β””β”€β”€ utils/            # μ ν‹Έλ¦¬ν‹° (BoardGenerator)
β”‚   β””β”€β”€ rl/                   # κ°•ν™”ν•™μµ λ¨λ“
β”‚       β”β”€β”€ env/
β”‚       β”‚   β””β”€β”€ color_tiles_env.py     # Gymnasium ν™κ²½
β”‚       β”β”€β”€ training/
β”‚       β”‚   β”β”€β”€ train.py               # ν•™μµ μ¤ν¬λ¦½νΈ
β”‚       β”‚   β””β”€β”€ callbacks.py           # μ»¤μ¤ν…€ μ½λ°±
β”‚       β””β”€β”€ inference/
β”‚           β””β”€β”€ ai_player.py           # AI ν”λ μ΄μ–΄
β”β”€β”€ tests/
β”‚   β””β”€β”€ test_color_tiles_env.py        # ν™κ²½ ν…μ¤νΈ
β”β”€β”€ checkpoints/              # ν•™μµλ λ¨λΈ μ €μ¥ (μλ™ μƒμ„±)
β”β”€β”€ logs/                     # TensorBoard λ΅κ·Έ (μλ™ μƒμ„±)
β”β”€β”€ docs/
β”‚   β””β”€β”€ reinforce_learning_plan.md     # RL κ³„νμ„
β”β”€β”€ main.py                   # GUI μ‹¤ν–‰ νμΌ
β”β”€β”€ README.md
β””β”€β”€ pyproject.toml
```

---

## κ°•ν™”ν•™μµ ν™κ²½ μ¤ν™

### State (κ΄€μ°° κ³µκ°„)

- **νƒ€μ…**: `Box(0, 10, (15, 23), int8)`
- **ν•νƒ**: 15Γ—23 2D κ·Έλ¦¬λ“
- **κ°’**:
  - 0: λΉ μ…€
  - 1-10: μƒ‰μƒ (Color enum value + 1)

### Action (ν–‰λ™ κ³µκ°„)

- **νƒ€μ…**: `Discrete(345)`
- **λ²”μ„**: 0-344 (23Γ—15 = 345κ° μ…€)
- **λ³€ν™**:
  - `row = action // 23`
  - `col = action % 23`

### Reward (λ³΄μƒ)

| μƒν™© | λ³΄μƒ |
|------|------|
| νƒ€μΌ μ κ±° | `+1.0 Γ— νƒ€μΌ μ` |
| λ¬΄ν¨ μ΄λ™ | `-10.0` |
| μΉλ¦¬ | `+100.0` |
| ν¨λ°° | `-(λ‚¨μ€ νƒ€μΌ Γ— 2)` |

### Episode μΆ…λ£

- **Terminated**: μΉλ¦¬ (λ¨λ“  νƒ€μΌ μ κ±°) λλ” ν¨λ°° (μ‹κ°„ μ΄κ³Ό/λ§‰ν)
- **Truncated**: Max steps (200) λ„λ‹¬

---

## PPO ν•μ΄νΌνλΌλ―Έν„°

```python
{
    "learning_rate": 3e-4,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 10,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
}
```

---

## μμƒ ν•™μµ μ§„ν–‰

| λ‹¨κ³„ | Steps | Win Rate | ν‰κ·  νƒ€μΌ μ κ±° | Invalid Move Rate |
|------|-------|----------|----------------|-------------------|
| μ΄κΈ° | 0-50K | 0-5% | 30-50 | 60-80% |
| μ΄μ¤‘κΈ° | 50K-200K | 5-30% | 80-120 | 30-50% |
| μ¤‘κΈ° | 200K-500K | 30-60% | 140-170 | 15-25% |
| ν›„κΈ° | 500K-1M | 60-80% | 175-195 | 5-10% |
| μλ ΄ | 1M+ | 80%+ | 195+ | <5% |

---

## μ²΄ν¬ν¬μΈνΈ κ΄€λ¦¬

ν•™μµ μ¤‘ μλ™μΌλ΅ λ‹¤μ μ²΄ν¬ν¬μΈνΈκ°€ μƒμ„±λ©λ‹λ‹¤:

```
checkpoints/
β”β”€β”€ ppo_colortiles_step_10000.zip    # 10K steps
β”β”€β”€ ppo_colortiles_step_20000.zip    # 20K steps
β”β”€β”€ ...
β”β”€β”€ ppo_colortiles_best.zip          # μµκ³  μ„±λ¥ λ¨λΈ
β””β”€β”€ ppo_colortiles_final.zip         # μµμΆ… λ¨λΈ
```

**κ¶μ¥ μ‚¬ν•­:**
- `best.zip`: ν‰κ°€ μ„±λ¥μ΄ κ°€μ¥ μΆ‹μ€ λ¨λΈ (GUIμ—μ„ μ‚¬μ© μ¶”μ²)
- `final.zip`: ν•™μµ μ™„λ£ ν›„ μµμΆ… λ¨λΈ
- `step_*.zip`: νΉμ • μ‹μ μ λ¨λΈ (ν•™μµ μ¬κ° μ‹ μ‚¬μ©)

---

## ν…μ¤νΈ

### ν™κ²½ ν…μ¤νΈ

```bash
pytest tests/test_color_tiles_env.py -v
```

### AI Player ν…μ¤νΈ

```bash
python -m rl.inference.ai_player checkpoints/ppo_colortiles_best.zip
```

---

## νΈλ¬λΈ”μν…

### 1. ModuleNotFoundError: No module named 'rl'

**λ¬Έμ **: Pythonμ΄ `rl` ν¨ν‚¤μ§€λ¥Ό μ°Ύμ§€ λ»ν•¨

**ν•΄κ²°**:
```bash
# ν”„λ΅μ νΈ λ£¨νΈμ—μ„
pip install -e .
```

### 2. stable-baselines3 not installed

**λ¬Έμ **: RL λΌμ΄λΈλ¬λ¦¬κ°€ μ„¤μΉλμ§€ μ•μ

**ν•΄κ²°**:
```bash
pip install gymnasium stable-baselines3 torch tensorboard
```

### 3. CUDA out of memory (GPU μ‚¬μ© μ‹)

**λ¬Έμ **: GPU λ©”λ¨λ¦¬ λ¶€μ΅±

**ν•΄κ²°**:
```bash
# CPU μ‚¬μ© κ°•μ 
export CUDA_VISIBLE_DEVICES=""
python -m rl.training.train ...
```

### 4. GUIμ—μ„ μ²΄ν¬ν¬μΈνΈκ°€ λ³΄μ΄μ§€ μ•μ

**λ¬Έμ **: `checkpoints/` λ””λ ‰ν† λ¦¬μ— νμΌμ΄ μ—†μ

**ν•΄κ²°**:
1. λ¨Όμ € ν•™μµμ„ μ‹¤ν–‰ν•μ—¬ μ²΄ν¬ν¬μΈνΈ μƒμ„±
2. GUIμ—μ„ "μƒλ΅κ³ μΉ¨" λ²„νΌ ν΄λ¦­

---

## μ„±λ¥ μµμ ν™”

### CPU ν•™μµ κ°€μ†

```bash
# λ³‘λ ¬ ν™κ²½ μ μ¦κ°€ (CPU μ½”μ–΄ μμ— λ§κ²)
python -m rl.training.train --n-envs 16
```

### GPU μ‚¬μ©

```bash
# PyTorchκ°€ μλ™μΌλ΅ GPU κ°μ§€
# device="auto"λ΅ μ„¤μ •λμ–΄ μμ
python -m rl.training.train --total-timesteps 1000000
```

---

## μ°Έκ³  λ¬Έμ„

- **RL κ³„νμ„**: `docs/reinforce_learning_plan.md`
- **Stable-Baselines3**: https://stable-baselines3.readthedocs.io/
- **Gymnasium**: https://gymnasium.farama.org/
- **PPO λ…Όλ¬Έ**: https://arxiv.org/abs/1707.06347

---

## λΌμ΄μ„Όμ¤

μ΄ ν”„λ΅μ νΈλ” κµμ΅ λ©μ μΌλ΅ μ μ‘λμ—μµλ‹λ‹¤.

---

## μ‘μ„±μ

- Color Tiles κ²μ„ μ—”μ§„: jmlee
- κ°•ν™”ν•™μµ ν†µν•©: Claude (Anthropic)

---

## λ²„μ „ νμ¤ν† λ¦¬

- **v1.0.0** (2025-12-07): μ΄κΈ° λ¦΄λ¦¬μ¤
  - PPO κΈ°λ° RL ν™κ²½ κµ¬ν„
  - ν•™μµ νμ΄ν”„λΌμΈ κµ¬ν„
  - GUI ν†µν•© μ™„λ£
